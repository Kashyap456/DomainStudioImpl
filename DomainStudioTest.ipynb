{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "G96qr-p0_y4P",
        "yhbCXNiwuJM3",
        "GIkJJNSRxeKe",
        "r1jphDAC7EK1",
        "qZwG-PjER1JY"
      ],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c1feac5fe8284a339f24d27fd572bf4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a16ec51010a2404c84881600e88c1760",
              "IPY_MODEL_d3188b7fd7dc4ff6ad92313659cb6121",
              "IPY_MODEL_52fb549496894aafb915835674304079"
            ],
            "layout": "IPY_MODEL_c2ff645d9b6f4ab0abb6d29425b5f05c"
          }
        },
        "a16ec51010a2404c84881600e88c1760": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca2df3c25878471799662366152be153",
            "placeholder": "​",
            "style": "IPY_MODEL_703c67653d4f4bc09b05c6fbfd67fe14",
            "value": "Epoch 0:  50%"
          }
        },
        "d3188b7fd7dc4ff6ad92313659cb6121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14131e3673ce4c4d832c048f40caccda",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96a3e1aa14de400880164a228126d722",
            "value": 1
          }
        },
        "52fb549496894aafb915835674304079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a859500071cc43e4a9d0867cd6d6f4a8",
            "placeholder": "​",
            "style": "IPY_MODEL_4c10d3ed540a4f8a905ad5b5bd5a0924",
            "value": " 1/2 [01:58&lt;01:58, 118.25s/it, loss=nan, lr=3.98e-6, step=0]"
          }
        },
        "c2ff645d9b6f4ab0abb6d29425b5f05c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca2df3c25878471799662366152be153": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "703c67653d4f4bc09b05c6fbfd67fe14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14131e3673ce4c4d832c048f40caccda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96a3e1aa14de400880164a228126d722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a859500071cc43e4a9d0867cd6d6f4a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c10d3ed540a4f8a905ad5b5bd5a0924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "import json\n",
        "import cv2"
      ],
      "metadata": {
        "id": "yX8afVI9AmrC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "p1yw_qraVcWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kJq2kSCaVyF",
        "outputId": "28b4c3fe-73b6-48fa-b38c-1ba811779290"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/domain_set/"
      ],
      "metadata": {
        "id": "MzDNP2rTnFzP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf __MACOSX/"
      ],
      "metadata": {
        "id": "hDRtq49nnaOO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/drive/MyDrive/domain_set.zip -d /content/"
      ],
      "metadata": {
        "id": "Cr9Qp2XgacOz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, folder_name):\n",
        "        self.data = []\n",
        "        self.folder = folder_name\n",
        "        with open(f\"./{folder_name}/prompt.json\", 'rt') as f:\n",
        "                self.data = json.load(f)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "\n",
        "        image_name = item['target']\n",
        "        prompt = item['prompt']\n",
        "\n",
        "        image = cv2.imread(f\"./{self.folder}/\" + image_name)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = (image.astype(np.float32) / 127.5) - 1.0\n",
        "\n",
        "        return dict(image=image, label_tr=f\"A [V] {prompt}\", label_so=f\"{prompt}\")"
      ],
      "metadata": {
        "id": "XKPYFLghVeNu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nagai_dataset = MyDataset(\"domain_set\")"
      ],
      "metadata": {
        "id": "m_XMsmJIX4qg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(nagai_dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "xTW0ZdifYEYA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Objectives"
      ],
      "metadata": {
        "id": "G96qr-p0_y4P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KHkyr5yY_hJU"
      },
      "outputs": [],
      "source": [
        "class Loss_Simple(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Loss_Simple, self).__init__()\n",
        "\n",
        "  def forward(self, y_ada, y_pretrain):\n",
        "    diff = y_ada - y_pretrain\n",
        "    return torch.mean(diff * diff)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss_PR(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Loss_PR, self).__init__()\n",
        "\n",
        "  def forward(self, y_pred_sou, y_pred_ada):\n",
        "    diff = y_pred_sou - y_pred_ada\n",
        "    return torch.mean(diff * diff)"
      ],
      "metadata": {
        "id": "loXW1a_aDj6m"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PairwiseSimilarityLoss(nn.Module):\n",
        "    def __init__(self, D):\n",
        "        super(PairwiseSimilarityLoss, self).__init__()\n",
        "        self.D = D\n",
        "        self.sim = F.cosine_similarity\n",
        "        self.sfm = F.softmax\n",
        "\n",
        "    def forward(self, z_ada, z_sou):\n",
        "      # Assume D represents a function that computes the denoised latent codes\n",
        "      denoised_ada = self.D(z_ada).sample\n",
        "      denoised_sou = self.D(z_sou).sample\n",
        "\n",
        "      # Calculate similarity scores using broadcasting for all unique pairs\n",
        "      # Expand dims to (batch_size, 1, features) for ada and (1, batch_size, features) for sou\n",
        "      # to compute pairwise similarity\n",
        "      # Reshape or flatten tensors to 2D (B, C*W*H)\n",
        "      denoised_ada_flat = denoised_ada.reshape(denoised_ada.size(0), -1)\n",
        "      denoised_sou_flat = denoised_sou.reshape(denoised_sou.size(0), -1)\n",
        "\n",
        "      # Compute cosine similarity for all pairs (B, B)\n",
        "      sim_matrix_ada = self.sim(denoised_ada_flat.unsqueeze(1), denoised_ada_flat.unsqueeze(0), dim=2)\n",
        "      sim_matrix_sou = self.sim(denoised_sou_flat.unsqueeze(1), denoised_sou_flat.unsqueeze(0), dim=2)\n",
        "\n",
        "      # Mask out the self-similarity (diagonal elements of the similarity matrix)\n",
        "      mask = torch.eye(sim_matrix_ada.size(0)).bool().to(sim_matrix_ada.device)\n",
        "      sim_matrix_ada.masked_fill_(mask, float('-inf'))\n",
        "      sim_matrix_sou.masked_fill_(mask, float('-inf'))\n",
        "\n",
        "      # Apply softmax to the non-diagonal elements to get the probabilities\n",
        "      p_ada = self.sfm(sim_matrix_ada, dim=1)\n",
        "      p_sou = self.sfm(sim_matrix_sou, dim=1)\n",
        "\n",
        "      # Calculate KL divergence\n",
        "      kl_divergence = F.kl_div(p_ada.log(), p_sou, reduction='batchmean')\n",
        "\n",
        "      return kl_divergence"
      ],
      "metadata": {
        "id": "s9xGUDMCE3am"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HaarWaveletTransform(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Define the 2D filters based on the outer product of the 1D filters\n",
        "        # Adjust the filters to have 3 output channels and 3 input channels\n",
        "        # The filters are of shape (out_channels, in_channels, height, width)\n",
        "        self.register_buffer('lh_filter', torch.tensor([[[1.0, -1.0], [1.0, -1.0]]]).repeat(3, 3, 1, 1) / 2.0)\n",
        "        self.register_buffer('hl_filter', torch.tensor([[[1.0, 1.0], [-1.0, -1.0]]]).repeat(3, 3, 1, 1) / 2.0)\n",
        "        self.register_buffer('hh_filter', torch.tensor([[[-1.0, 1.0], [1.0, -1.0]]]).repeat(3, 3, 1, 1) / 2.0)\n",
        "\n",
        "        # Adjust filters to have 3 input channels\n",
        "        for i in range(3):\n",
        "          self.lh_filter[i, :, :, :] *= torch.tensor(i == 0).float()\n",
        "          self.hl_filter[i, :, :, :] *= torch.tensor(i == 1).float()\n",
        "          self.hh_filter[i, :, :, :] *= torch.tensor(i == 2).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ensure input x has a batch and channel dimension\n",
        "        if x.ndim == 3:\n",
        "            x = x.unsqueeze(1)  # Add a batch dimension if it's not present\n",
        "\n",
        "        # Apply filters to input x\n",
        "        lh = F.conv2d(x, self.lh_filter, stride=2)\n",
        "        hl = F.conv2d(x, self.hl_filter, stride=2)\n",
        "        hh = F.conv2d(x, self.hh_filter, stride=2)\n",
        "\n",
        "        # Sum the results from each filter\n",
        "        return lh + hl + hh\n",
        "\n",
        "\n",
        "\n",
        "# Example usage:\n",
        "# Assume `image` is a PyTorch tensor of shape (1, height, width) representing a grayscale image\n",
        "# The image tensor should have a batch dimension as well, so the full shape would be (batch_size, 1, height, width)\n",
        "#haar_wavelet_transform = HaarWaveletTransform()\n",
        "\n",
        "# Apply the transform to the image\n",
        "#lh, hl, hh = haar_wavelet_transform(image.unsqueeze(0))"
      ],
      "metadata": {
        "id": "rNB6oYlGOIdv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss_HF(nn.Module):\n",
        "  def __init__(self, D):\n",
        "    super(Loss_HF, self).__init__()\n",
        "    self.hwt = HaarWaveletTransform()\n",
        "    self.sim = F.cosine_similarity\n",
        "    self.sfm = F.softmax\n",
        "    self.D = D\n",
        "\n",
        "  def forward(self, z_ada, z_sou):\n",
        "      # Assume D represents a function that computes the denoised latent codes\n",
        "      # use the transform to extract the high frequency (fine-grained) details\n",
        "      denoised_ada = self.hwt(self.D(z_ada).sample)\n",
        "      denoised_sou = self.hwt(self.D(z_sou).sample)\n",
        "\n",
        "      # Calculate similarity scores using broadcasting for all unique pairs\n",
        "      # Expand dims to (batch_size, 1, features) for ada and (1, batch_size, features) for sou\n",
        "      # to compute pairwise similarity\n",
        "      denoised_ada_flat = denoised_ada.reshape(denoised_ada.size(0), -1)\n",
        "      denoised_sou_flat = denoised_sou.reshape(denoised_sou.size(0), -1)\n",
        "\n",
        "      # Compute cosine similarity for all pairs (B, B)\n",
        "      sim_matrix_ada = self.sim(denoised_ada_flat.unsqueeze(1), denoised_ada_flat.unsqueeze(0), dim=2)\n",
        "      sim_matrix_sou = self.sim(denoised_sou_flat.unsqueeze(1), denoised_sou_flat.unsqueeze(0), dim=2)\n",
        "\n",
        "      # Mask out the self-similarity (diagonal elements of the similarity matrix)\n",
        "      mask = torch.eye(sim_matrix_ada.size(0)).bool().to(sim_matrix_ada.device)\n",
        "      sim_matrix_ada.masked_fill_(mask, float('-inf'))\n",
        "      sim_matrix_sou.masked_fill_(mask, float('-inf'))\n",
        "\n",
        "      # Apply softmax to the non-diagonal elements to get the probabilities\n",
        "      p_ada = self.sfm(sim_matrix_ada, dim=1)\n",
        "      p_sou = self.sfm(sim_matrix_sou, dim=1)\n",
        "\n",
        "      # Calculate KL divergence\n",
        "      kl_divergence = F.kl_div(p_ada.log(), p_sou, reduction='batchmean')\n",
        "\n",
        "      return kl_divergence"
      ],
      "metadata": {
        "id": "zJPtS0nSpRDZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss_HFMSE(nn.Module):\n",
        "  def __init__(self, D):\n",
        "    super(Loss_HFMSE, self).__init__()\n",
        "    self.hwt = HaarWaveletTransform()\n",
        "    self.D = D\n",
        "\n",
        "  def forward(self, z_ada, x_init):\n",
        "    diff = self.hwt(self.D(z_ada).sample) - self.hwt(x_init)\n",
        "    return torch.mean(diff * diff)"
      ],
      "metadata": {
        "id": "EAbbn57Zq2Yo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DomainLoss(nn.Module):\n",
        "  def __init__(self, D):\n",
        "    super(DomainLoss, self).__init__()\n",
        "    #Loss Components\n",
        "    self.l_simp = Loss_Simple()\n",
        "    self.l_pr = Loss_PR()\n",
        "    self.l_img = PairwiseSimilarityLoss(D)\n",
        "    self.l_hf = Loss_HF(D)\n",
        "    self.l_hfmse = Loss_HFMSE(D)\n",
        "\n",
        "    #Loss Weights\n",
        "    self.l1 = 1\n",
        "    self.l2 = 2.5e+2\n",
        "    self.l3 = 2.5e+2\n",
        "    self.l4 = 0.6\n",
        "  def forward(self, z_ada, z, z_pr_sou, z_pr_ada, x_init):\n",
        "    return self.l_simp(z_ada, z) + self.l1 * self.l_pr(z_pr_sou, z_pr_ada) \\\n",
        "    + self.l2 * self.l_img(z_ada, z_pr_ada) + self.l3 * self.l_hf(z_ada, z_pr_ada) * self.l_hfmse(z_ada, x_init)"
      ],
      "metadata": {
        "id": "AXvoRw2brRem"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "yhbCXNiwuJM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Imports"
      ],
      "metadata": {
        "id": "QDtRm4rRxaDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade diffusers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QtRq30Lxcnt",
        "outputId": "d631f08a-f3a5-454a-8761-2fd1ad9640a2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diffusers[torch] in /usr/local/lib/python3.10/dist-packages (0.26.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers[torch]) (7.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.2 in /usr/local/lib/python3.10/dist-packages (from diffusers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers[torch]) (1.23.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers[torch]) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers[torch]) (0.4.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers[torch]) (9.4.0)\n",
            "Requirement already satisfied: torch<2.2.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from diffusers[torch]) (2.1.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from diffusers[torch]) (0.26.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.11.0->diffusers[torch]) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.11.0->diffusers[torch]) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.11.0->diffusers[torch]) (6.0.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers[torch]) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.2->diffusers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.4->diffusers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.4->diffusers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.4->diffusers[torch]) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.2.0,>=1.4->diffusers[torch]) (2.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers[torch]) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers[torch]) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<2.2.0,>=1.4->diffusers[torch]) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.2.0,>=1.4->diffusers[torch]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Component Instantiation"
      ],
      "metadata": {
        "id": "GIkJJNSRxeKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, LMSDiscreteScheduler"
      ],
      "metadata": {
        "id": "An5CAsy3xRQU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"runwayml/stable-diffusion-v1-5\""
      ],
      "metadata": {
        "id": "4gHQqkjHyELf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "Encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6IUzzILzXVc",
        "outputId": "34a8040c-a1fa-44f1-c34c-28791b1be454"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VAE = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\")"
      ],
      "metadata": {
        "id": "1DLEZLsyz9xF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "UNetLocked = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\")\n",
        "UNetTrained = UNet2DConditionModel.from_pretrained(model_id, subfolder=\"unet\")"
      ],
      "metadata": {
        "id": "niY-bGntuKde"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Scheduler = LMSDiscreteScheduler(\n",
        "    beta_start = 0.00085,\n",
        "    beta_end = 0.012,\n",
        "    beta_schedule = 'scaled_linear',\n",
        "    num_train_timesteps = 1000\n",
        ");"
      ],
      "metadata": {
        "id": "58Wk805rFBv5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Loss = DomainLoss(VAE.decode)"
      ],
      "metadata": {
        "id": "CGQzM3_rJj-u"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "num_epochs = 1250 // (batch_size * 2)"
      ],
      "metadata": {
        "id": "gQMqw08DVCzL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Optimizer = optim.Adam(UNetTrained.parameters(), lr=4e-6)"
      ],
      "metadata": {
        "id": "q20sCMoPSll5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_scheduler = torch.optim.swa_utils.SWALR(Optimizer, anneal_strategy=\"linear\", anneal_epochs=num_epochs, swa_lr=1.5e-6)"
      ],
      "metadata": {
        "id": "DWxNyFjwU1gs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "NLDyzMcE4H9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "from tqdm.auto import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "n7bvzewQ71pb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Utils"
      ],
      "metadata": {
        "id": "r1jphDAC7EK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from dataclasses import dataclass\n",
        "\n",
        "#\n",
        "# class TrainingConfig:\n",
        "#     image_size = 128  # the generated image resolution\n",
        "#     train_batch_size = 16\n",
        "#     eval_batch_size = 16  # how many images to sample during evaluation\n",
        "#     num_epochs = 50\n",
        "#     gradient_accumulation_steps = 1\n",
        "#     learning_rate = 1e-4\n",
        "#     lr_warmup_steps = 500\n",
        "#     save_image_epochs = 10\n",
        "#     save_model_epochs = 30\n",
        "#     mixed_precision = 'fp16'  # `no` for float32, `fp16` for automatic mixed precision\n",
        "#     output_dir = 'ddpm-butterflies-128'  # the model namy locally and on the HF Hub\n",
        "\n",
        "#     push_to_hub = False  # whether to upload the saved model to the HF Hub\n",
        "#     hub_private_repo = False\n",
        "#     overwrite_output_dir = True  # overwrite the old model when re-running the notebook\n",
        "#     seed = 0\n",
        "\n",
        "# config = TrainingConfig()"
      ],
      "metadata": {
        "id": "Dqejbbgr4HrP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "8tW4Owcr7wKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boilerplate for later"
      ],
      "metadata": {
        "id": "qZwG-PjER1JY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
        "#     # Initialize accelerator and tensorboard logging\n",
        "#     accelerator = Accelerator(\n",
        "#         mixed_precision=config.mixed_precision,\n",
        "#         gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "#         log_with=\"tensorboard\",\n",
        "#         logging_dir=os.path.join(config.output_dir, \"logs\")\n",
        "#     )\n",
        "#     if accelerator.is_main_process:\n",
        "#         accelerator.init_trackers(\"train_example\")\n",
        "\n",
        "#     # Prepare everything\n",
        "#     # There is no specific order to remember, you just need to unpack the\n",
        "#     # objects in the same order you gave them to the prepare method.\n",
        "#     model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
        "#         model, optimizer, train_dataloader, lr_scheduler\n",
        "#     )\n",
        "\n",
        "#     global_step = 0\n",
        "\n",
        "#     # Now you train the model\n",
        "#     for epoch in range(config.num_epochs):\n",
        "#         progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)\n",
        "#         progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "#         for step, batch in enumerate(train_dataloader):\n",
        "#             images = batch['images']\n",
        "#             x_pr = torch.randn(images.shape).to(images.device)\n",
        "\n",
        "#             #create c_tar using clip\n",
        "#             labels_tr = batch['labels_tr']\n",
        "#             tokens_tr = Tokenizer(labels_tr, padding=True, return_tensors=\"pt\")\n",
        "#             c_tar = Encoder(**tokens_tr).last_hidden_state\n",
        "\n",
        "#             #create c_sou using clip\n",
        "#             labels_so = batch['labels_so']\n",
        "#             tokens_so = Tokenizer(labels_so, padding=True, return_tensors=\"pt\")\n",
        "#             c_sou = Encoder(**tokens_so).last_hidden_state\n",
        "\n",
        "#             # Sample noise to add to the images\n",
        "#             z = VAE.encode(images)\n",
        "#             z_pr = VAE.encode(x_pr)\n",
        "#             noise = torch.randn(z.shape).to(z.device)\n",
        "#             bs = z.shape[0]\n",
        "\n",
        "#             # Sample a random timestep for each image\n",
        "#             timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=z.device).long()\n",
        "\n",
        "#             # Add noise to the clean images according to the noise magnitude at each timestep\n",
        "#             # (this is the forward diffusion process)\n",
        "#             z_t = noise_scheduler.add_noise(z, noise, timesteps)\n",
        "\n",
        "#             # Get the random noise z_pr_t\n",
        "#             z_pr_t = noise_scheduler.add_noise(z_pr, noise, timesteps)\n",
        "\n",
        "#             with torch.no_grad():\n",
        "#                 # Predict the noise residual\n",
        "#                 z_pr_sou = UNetLocked(z_pr_t, timesteps, c_sou)\n",
        "\n",
        "#             with accelerator.accumulate(model):\n",
        "#                 # Predict the noise residual\n",
        "#                 z_ada = model(z_t, timesteps, c_tar)[\"sample\"]\n",
        "#                 z_pr_ada = model(z_pr_t, timesteps, c_sou)[\"sample\"]\n",
        "#                 loss = Loss(z_ada, z, z_pr_sou, z_pr_ada, images)\n",
        "#                 accelerator.backward(loss)\n",
        "\n",
        "#                 accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
        "#                 optimizer.step()\n",
        "#                 lr_scheduler.step()\n",
        "#                 optimizer.zero_grad()\n",
        "\n",
        "#             progress_bar.update(1)\n",
        "#             logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "#             progress_bar.set_postfix(**logs)\n",
        "#             accelerator.log(logs, step=global_step)\n",
        "#             global_step += 1"
      ],
      "metadata": {
        "id": "ItpBNEJV7xxX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Training Loop"
      ],
      "metadata": {
        "id": "0WCih64mR9HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now you train the model\n",
        "global_step = 0\n",
        "for epoch in range(1250 // batch_size):\n",
        "    progress_bar = tqdm(total=len(train_dataloader))\n",
        "    progress_bar.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        images = batch['image']\n",
        "        images = images.permute(0, 3, 1, 2)\n",
        "        x_pr = torch.randn(images.shape).to(images.device)\n",
        "\n",
        "        #create c_tar using clip\n",
        "        labels_tr = batch['label_tr']\n",
        "        tokens_tr = Tokenizer(labels_tr, padding=True, return_tensors=\"pt\")\n",
        "        c_tar = Encoder(**tokens_tr).last_hidden_state\n",
        "\n",
        "        #create c_sou using clip\n",
        "        labels_so = batch['label_so']\n",
        "        tokens_so = Tokenizer(labels_so, padding=True, return_tensors=\"pt\")\n",
        "        c_sou = Encoder(**tokens_so).last_hidden_state\n",
        "\n",
        "        # Sample noise to add to the images\n",
        "        z = VAE.encode(images).latent_dist.sample()\n",
        "        z_pr = VAE.encode(x_pr).latent_dist.sample()\n",
        "        noise = torch.randn(z.shape).to(z.device)\n",
        "        bs = z.shape[0]\n",
        "\n",
        "        # Sample a random timestep for each image\n",
        "        timesteps = torch.randint(0, Scheduler.num_train_timesteps, (bs,), device=z.device).long()\n",
        "\n",
        "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
        "        # (this is the forward diffusion process)\n",
        "        z_t = Scheduler.add_noise(z, noise, timesteps)\n",
        "\n",
        "        # Get the random noise z_pr_t\n",
        "        z_pr_t = Scheduler.add_noise(z_pr, noise, timesteps)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Predict the noise residual\n",
        "            z_pr_sou = UNetLocked(z_pr_t, timesteps, c_sou)[\"sample\"]\n",
        "\n",
        "        # Predict the noise residual\n",
        "        z_ada = UNetTrained(z_t, timesteps, c_tar)[\"sample\"]\n",
        "        z_pr_ada = UNetTrained(z_pr_t, timesteps, c_sou)[\"sample\"]\n",
        "        loss = Loss(z_ada, z, z_pr_sou, z_pr_ada, images)\n",
        "        loss.backward()\n",
        "        Optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        Optimizer.zero_grad()\n",
        "\n",
        "        progress_bar.update(1)\n",
        "        logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
        "        progress_bar.set_postfix(**logs)\n",
        "        global_step += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487,
          "referenced_widgets": [
            "c1feac5fe8284a339f24d27fd572bf4b",
            "a16ec51010a2404c84881600e88c1760",
            "d3188b7fd7dc4ff6ad92313659cb6121",
            "52fb549496894aafb915835674304079",
            "c2ff645d9b6f4ab0abb6d29425b5f05c",
            "ca2df3c25878471799662366152be153",
            "703c67653d4f4bc09b05c6fbfd67fe14",
            "14131e3673ce4c4d832c048f40caccda",
            "96a3e1aa14de400880164a228126d722",
            "a859500071cc43e4a9d0867cd6d6f4a8",
            "4c10d3ed540a4f8a905ad5b5bd5a0924"
          ]
        },
        "id": "zpfd1vW3R0SQ",
        "outputId": "71aeedf5-cb33-4e3f-b463-2f85c69e9131"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1feac5fe8284a339f24d27fd572bf4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/diffusers/configuration_utils.py:139: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'LMSDiscreteScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'LMSDiscreteScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
            "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-e95a4fea6fe5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Predict the noise residual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mz_pr_sou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNetLocked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_pr_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_sou\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sample\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Predict the noise residual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/unets/unet_2d_condition.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupsample_block\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"has_cross_attention\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mupsample_block\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_cross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m                 sample = upsample_block(\n\u001b[0m\u001b[1;32m   1187\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m                     \u001b[0mtemb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/unets/unet_2d_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, res_hidden_states_tuple, temb, encoder_hidden_states, cross_attention_kwargs, upsample_size, attention_mask, encoder_attention_mask)\u001b[0m\n\u001b[1;32m   2420\u001b[0m                 )[0]\n\u001b[1;32m   2421\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2422\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlora_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2423\u001b[0m                 hidden_states = attn(\n\u001b[1;32m   2424\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor, temb, scale)\u001b[0m\n\u001b[1;32m    365\u001b[0m             )\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mUSE_PEFT_BACKEND\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_emb_proj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/diffusers/models/lora.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, scale)\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# make sure to the functional Conv2D function as otherwise torch.compile's graph will break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;31m# see: https://github.com/huggingface/diffusers/pull/4315\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             return F.conv2d(\n\u001b[0m\u001b[1;32m    359\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m             )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}